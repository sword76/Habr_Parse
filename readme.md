# Парсер статей с Habr.com

Python-скрипт для сбора данных о популярных статьях с сайта Habr.com из раздела "Бэкенд" (топ за день).

## Функциональность

- Парсинг списка статей из раздела "Бэкенд" (топ за день)
- Сбор основной информации о статьях: заголовок, URL, количество просмотров
- Извлечение полного текста каждой статьи
- Структурированное хранение данных с использованием dataclasses
- Автоматическая генерация случайных User-Agent для обхода блокировок

## Предварительные требования

- Python 3.7+
- Доступ к сайту habr.com

## Установка

### 1. Клонирование репозитория

```bash
git clone https://github.com/your-username/habr-parser.git
cd habr-parser
```

### 2. Создание виртуального окружения

```bash
python -m venv venv
source venv/bin/activate  # Linux/MacOS
venv\Scripts\activate     # Windows
```

### 3. Установка зависимостей

```bash
pip install -r requirements.txt
```

**Необходимые пакеты:**
- `requests` - для выполнения HTTP-запросов
- `beautifulsoup4` - для парсинга HTML
- `lxml` - парсер для BeautifulSoup
- `fake-useragent` - для генерации случайных User-Agent

Файл `requirements.txt` должен содержать:
```
requests>=2.28.0
beautifulsoup4>=4.11.0
lxml>=4.9.0
fake-useragent>=1.1.0
```

## Использование

### Запуск парсера

```bash
python main.py
```

### Настройка целевого URL

По умолчанию скрипт парсит статьи из раздела:
```python
url = 'https://habr.com/ru/flows/backend/articles/top/daily/'
```

Вы можете изменить URL для парсинга других разделов Habr:
- `.../flows/develop/...` - Разработка
- `.../flows/admin/...` - Администрирование
- `.../flows/design/...` - Дизайн

## Структура проекта

```
habr-parser/
├── main.py      # Основной скрипт парсера
├── requirements.txt    # Зависимости проекта
├── .gitignore         # Игнорируемые файлы
└── README.md          # Документация
```

## Структура данных

Каждая статья сохраняется в виде датакласса `ArticleData`:

```python
@dataclass
class ArticleData:
    title: str    # Заголовок статьи
    views: str    # Количество просмотров
    URL: str      # Полная ссылка на статью
    text: str     # Полный текст статьи
```

## Особенности реализации

### Обработка ошибок

- Автоматическая проверка статуса HTTP-ответов
- Обработка исключений при получении текста статей
- Генерация понятных сообщений об ошибках

### Обход ограничений

- Использование случайных User-Agent для имитации реальных браузеров
- Задержки между запросами для соблюдения правил сайта
- Корректная обработка отсутствующих элементов на странице

### Производительность

- Оптимизированный парсинг с использованием lxml
- Последовательная загрузка статей для снижения нагрузки на сервер
- Эффективное извлечение текста с сохранением форматирования

## Пример вывода

Скрипт выводит структурированные данные о статьях:

```python
[ArticleData(title='Название статьи 1', views='15420', URL='https://habr.com/ru/post/123456/', text='Полный текст статьи...'),
 ArticleData(title='Название статьи 2', views='8920', URL='https://habr.com/ru/post/123457/', text='Полный текст статьи...')]
```

## Важные замечания

### Соблюдение правил Habr.com

- Скрипт включает задержки между запросами (0.5 секунд)
- Используется корректные HTTP-заголовки
- Рекомендуется не превышать разумные лимиты запросов

### Ограничения

- Парсинг только статей из топа за день
- Зависимость от структуры HTML Habr.com (при изменениях потребуется обновление)
- Однопоточное выполнение для соблюдения правил сайта

### Рекомендации по использованию

1. **Для личного использования**: Запускайте скрипт не чаще 1-2 раз в день
2. **Для продакшн**: Добавьте планировщик задач с интервалом 24 часа
3. **Масштабирование**: Рассмотрите добавление многопоточности с осторожностью

## Расширение функциональности

### Сохранение в файл

Добавьте функцию для сохранения данных:

```python
import json
from dataclasses import asdict

def save_to_file(articles: list[ArticleData], filename: str = "habr_articles.json"):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump([asdict(article) for article in articles], f, ensure_ascii=False, indent=2)
```

### Фильтрация по просмотрам

```python
def filter_popular_articles(articles: list[ArticleData], min_views: int = 10000):
    return [article for article in articles if int(article.views.replace(' ', '')) > min_views]
```

## Устранение неполадок

### Проблемы с подключением

```python
# Увеличьте таймаут запросов
res = requests.get(url, headers=headers, timeout=30)
```

### Изменения структуры сайта

Если скрипт перестал работать:
1. Проверьте актуальные CSS-классы на Habr.com
2. Обновите селекторы в функциях парсинга
3. Протестируйте на небольшом количестве статей

### Блокировка запросов

- Убедитесь, что fake-useragent генерирует актуальные заголовки
- Добавьте дополнительные задержки между запросами
- Рассмотрите использование прокси-серверов

## Лицензия

MIT License

## Вклад в проект

1. Форкните репозиторий
2. Создайте ветку для новой функциональности
3. Закоммитьте изменения
4. Откройте Pull Request

## Поддержка

При возникновении проблем:
- Проверьте работу сайта habr.com
- Убедитесь в актуальности версий библиотек
- Проверьте интернет-подключение
- Ознакомьтесь с логами выполнения скрипта
